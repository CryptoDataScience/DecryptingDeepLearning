{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7001f184",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "#### Pytorch Lightning, HuggingFace Bert, Pandas, SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1796640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2512405b730>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics.functional import auroc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c4534",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce712d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read in the dataset\n",
    "toxiccomments_df = pd.read_csv(\"toxic_comments.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e77c278",
   "metadata": {},
   "source": [
    "### Text to Numeric Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee882f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This method is a batch method to load the augmented data files\n",
    "import re\n",
    "\n",
    "def process_file_to_dataframe(file_path):\n",
    "    # Regular expression to match lines with six comma-separated integers (either 0 or 1) at the end\n",
    "    pattern = re.compile(r\"(.*),(0|1),\\s*(0|1),\\s*(0|1),\\s*(0|1),\\s*(0|1),\\s*(0|1)$\")\n",
    "\n",
    "    # Preparing a list to store each row as a dictionary\n",
    "    data = []\n",
    "\n",
    "    # Reading the file and processing each line\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    # Creating a dictionary for each matched line\n",
    "                    row = {\n",
    "                        'comment_text': match.group(1).strip(),\n",
    "                        'toxic': int(match.group(2)),\n",
    "                        'severe_toxic': int(match.group(3)),\n",
    "                        'obscene': int(match.group(4)),\n",
    "                        'threat': int(match.group(5)),\n",
    "                        'insult': int(match.group(6)),\n",
    "                        'identity_hate': int(match.group(7))\n",
    "                    }\n",
    "                    data.append(row)\n",
    "    except UnicodeDecodeError:\n",
    "        # Try a different encoding in case of a decode error\n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "            for line in file:\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    # Creating a dictionary for each matched line\n",
    "                    row = {\n",
    "                        'comment_text': match.group(1).strip(),\n",
    "                        'toxic': int(match.group(2)),\n",
    "                        'severe_toxic': int(match.group(3)),\n",
    "                        'obscene': int(match.group(4)),\n",
    "                        'threat': int(match.group(5)),\n",
    "                        'insult': int(match.group(6)),\n",
    "                        'identity_hate': int(match.group(7))\n",
    "                    }\n",
    "                    data.append(row)\n",
    "\n",
    "    # Creating a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "file_path = 'translated_text_with_labels_fr.txt'   # Replace with your actual file path\n",
    "augdata_fr = process_file_to_dataframe(file_path)\n",
    "\n",
    "# Now 'df' is a pandas DataFrame with the specified structure\n",
    "augdata_fr.insert(0, 'id', 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea0c4a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'other_df' is your other DataFrame\n",
    "# Append 'df' to 'other_df'\n",
    "##toxiccomments_df = pd.concat([toxiccomments_df, augdata_ger], ignore_index=True)\n",
    "##toxiccomments_df = pd.concat([toxiccomments_df, augdata_sp], ignore_index=True)\n",
    "toxiccomments_df = pd.concat([toxiccomments_df, augdata_fr], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4730737",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "See Data Augmentation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0474f",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Splitting up Data\n",
    "Creating Pytorch DataSet Class\n",
    "Creating Pytorch DataLoader Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6efefed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split up our data\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initial split: 90% train, 10% temporary (for validation and test)\n",
    "train_df, temp_df = train_test_split(toxiccomments_df, test_size=0.1)\n",
    "\n",
    "# Second split of the temporary dataset: 50% validation, 50% test (5% of the original dataset each)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ca156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'other_df' is your other DataFrame\n",
    "# Append 'df' to 'other_df'\n",
    "##toxiccomments_df = pd.concat([toxiccomments_df, augdata_ger], ignore_index=True)\n",
    "##toxiccomments_df = pd.concat([toxiccomments_df, augdata_sp], ignore_index=True)\n",
    "toxiccomments_df = pd.concat([toxiccomments_df, augdata_fr], ignore_index=True)\n",
    "# Now 'appended_df' is the combined DataFrame\n",
    "print(toxiccomments_df.head())  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dec6f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((158090, 8), (8783, 8), (8783, 8))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Inspect the size of our datasets\n",
    "\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd4be75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  id                                       comment_text  \\\n",
      "0   0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
      "2   000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
      "3   0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
      "8   00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...   \n",
      "10  0005300084f90edc  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...   \n",
      "\n",
      "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0       0             0        0       0       0              0  \n",
      "2       0             0        0       0       0              0  \n",
      "3       0             0        0       0       0              0  \n",
      "8       0             0        0       0       0              0  \n",
      "10      0             0        0       0       0              0  \n"
     ]
    }
   ],
   "source": [
    "long_strings = toxiccomments_df[toxiccomments_df['comment_text'].str.len() > 128]\n",
    "\n",
    "# Display the result\n",
    "print(long_strings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1f65211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_c = toxiccomments_df.columns.tolist()[2:]\n",
    "\n",
    "summed_data = toxiccomments_df[labels_c].sum().sort_values()\n",
    "\n",
    "# Getting the colors from the rainbow palette, one for each bar\n",
    "palette = sns.color_palette(\"rainbow\", len(summed_data))\n",
    "\n",
    "# Creating the bar plot with individual colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(y=summed_data.index, x=summed_data.values, palette=palette)\n",
    "plt.title('Record Counts')\n",
    "plt.xlabel('Records')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f7af761",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_toxic = train_df[train_df[labels_c].sum(axis=1) > 0]\n",
    "train_clean = train_df[train_df[labels_c].sum(axis=1) == 0]\n",
    "\n",
    "\n",
    "train_df = pd.concat([  train_toxic,  train_clean.sample(30000)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdfb8797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59110"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83853493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14650"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ebcebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME, return_dict = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e87233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_count = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5b5c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pytorch Dataset Class\n",
    "##need to define our getitem in our pytorch dataset\n",
    "##provide parameters in how to load the data\n",
    "\n",
    "class ToxicCommentsDataset(Dataset):\n",
    "\n",
    "  def __init__(\n",
    "    self, \n",
    "    data: pd.DataFrame, \n",
    "    tokenizer: BertTokenizer, \n",
    "    max_token_len: int = 128\n",
    "  ):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data = data\n",
    "    self.max_token_len = max_token_len\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index: int):\n",
    "    \n",
    "    ##gets a single row\n",
    "    data_row = self.data.iloc[index]\n",
    "\n",
    "    comment_text = data_row.comment_text\n",
    "    labels = data_row[labels_c]\n",
    "    \n",
    "    ##parameters for Bert\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      comment_text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_token_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    return dict(        \n",
    "      ##text inputids, attention  \n",
    "      comment_text=comment_text,\n",
    "      input_ids=encoding[\"input_ids\"].flatten(), ##remove excess dimension\n",
    "      attention_mask=encoding[\"attention_mask\"].flatten(), ##remove excess dimension\n",
    "      labels=torch.FloatTensor(labels)) ##float tensor required by loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69de1b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ToxicCommentsDataset(\n",
    "  train_df,\n",
    "  tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4750e9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.4395,  0.2536, -0.0164,  ..., -0.1966,  0.5110,  0.0689],\n",
      "         [ 0.6004, -0.2250,  0.2992,  ..., -0.1561,  0.1168,  0.1942],\n",
      "         [-0.0699, -0.1049,  0.0520,  ...,  0.0523, -0.1588,  0.2129],\n",
      "         ...,\n",
      "         [ 0.1794, -0.1084, -0.1187,  ...,  0.6084,  0.3537, -0.0576],\n",
      "         [ 0.5189, -0.0482,  0.1877,  ..., -0.0955, -0.0479,  0.2421],\n",
      "         [ 0.1709,  0.4509, -0.0793,  ..., -0.3501,  0.9191, -0.5921]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7063,  0.4875,  0.9999, -0.9938,  0.9680,  0.9220,  0.9903, -0.9924,\n",
      "         -0.9803, -0.6939,  0.9851,  0.9986, -0.9986, -0.9998,  0.8586, -0.9822,\n",
      "          0.9883, -0.5301, -0.9999, -0.8283, -0.4781, -0.9998,  0.2469,  0.9802,\n",
      "          0.9793,  0.0585,  0.9864,  0.9999,  0.9191, -0.3221,  0.2377, -0.9925,\n",
      "          0.8780, -0.9985,  0.1314,  0.1070,  0.8019, -0.1271,  0.8201, -0.9463,\n",
      "         -0.7594, -0.7919,  0.7308, -0.5193,  0.9426,  0.3260,  0.1086,  0.0143,\n",
      "          0.0086,  0.9996, -0.9726,  0.9972, -0.9961,  0.9950,  0.9963,  0.5145,\n",
      "          0.9963,  0.1480, -0.9989,  0.1614,  0.9686,  0.2540,  0.9252, -0.0617,\n",
      "          0.3165, -0.3044, -0.9278,  0.0746, -0.4813,  0.1865,  0.3903,  0.3203,\n",
      "          0.9885, -0.9065, -0.0274, -0.8891,  0.2370, -0.9999,  0.9626,  0.9999,\n",
      "          0.7922, -0.9997,  0.9951, -0.2285, -0.7606,  0.7269, -0.9988, -0.9994,\n",
      "          0.1570, -0.6542,  0.9277, -0.9874,  0.6553, -0.8957,  1.0000, -0.9638,\n",
      "         -0.1379,  0.3217,  0.9636, -0.8004, -0.7704,  0.9605,  0.9990, -0.9940,\n",
      "          0.9985,  0.7958, -0.9495, -0.8757,  0.8176,  0.0170,  0.9910, -0.9899,\n",
      "         -0.8674, -0.0546,  0.9487, -0.9165,  0.9906,  0.8761, -0.2368,  1.0000,\n",
      "         -0.1574,  0.9640,  0.9986,  0.8906, -0.8660, -0.2064, -0.6213,  0.9000,\n",
      "         -0.5331, -0.3313,  0.7702, -0.9919, -0.9981,  0.9995, -0.3318,  1.0000,\n",
      "         -0.9991,  0.9937, -0.9999, -0.8476, -0.8504, -0.0575, -0.9882,  0.1657,\n",
      "          0.9929,  0.0557, -0.9597, -0.8449,  0.6332, -0.9076,  0.5182,  0.6990,\n",
      "         -0.9669,  0.9936,  0.9975,  0.9490,  0.9894,  0.1861, -0.9563,  0.9324,\n",
      "          0.9832, -0.9994,  0.7557, -0.9937,  0.9991,  0.9760,  0.7713, -0.9956,\n",
      "          0.9999, -0.6781, -0.0874, -0.0600, -0.1904, -0.9989,  0.4935,  0.4065,\n",
      "          0.7936,  0.9993, -0.9963,  0.9993,  0.9004,  0.0942,  0.7508,  0.9988,\n",
      "         -0.9963, -0.9836, -0.9879,  0.2017,  0.6631,  0.7994,  0.4239,  0.9676,\n",
      "          0.9984,  0.6998, -0.9977, -0.3786,  0.9772, -0.1280,  1.0000, -0.3250,\n",
      "         -0.9997, -0.8273,  0.9478,  0.9910, -0.3235,  0.9861, -0.7240, -0.2742,\n",
      "          0.9878, -0.9901,  0.9980,  0.1145,  0.8936,  0.9010,  0.9949, -0.8812,\n",
      "         -0.1006,  0.1588, -0.7661,  0.9999, -0.9995, -0.3067,  0.4703, -0.9928,\n",
      "         -0.9982,  0.9886,  0.0542, -0.9078, -0.1935,  0.7706,  0.1807,  0.9350,\n",
      "          0.9908, -0.6531, -0.6214, -0.9998, -0.9977, -0.8635, -0.9612,  0.0188,\n",
      "          0.6283, -0.3339, -0.9099, -0.9979,  0.9766,  0.8531, -0.9393, -0.1225,\n",
      "         -0.6998, -0.9982,  0.6342, -0.8804, -0.9983,  0.9994, -0.8295,  0.9965,\n",
      "          0.9814, -0.9952,  0.8586, -0.9988, -0.0666, -0.9948,  0.3440,  0.6532,\n",
      "         -0.8286, -0.0044,  0.9924, -0.9729, -0.8682,  0.8484, -0.9999,  0.9495,\n",
      "         -0.1826,  0.9991,  0.8558,  0.1308,  0.9870,  0.9288, -0.9829, -0.9998,\n",
      "          0.9363,  0.9441, -0.9942, -0.1393,  0.9999, -0.9981, -0.8287, -0.9538,\n",
      "         -0.9962, -0.9996,  0.2705, -0.8602,  0.1838,  0.9871,  0.3848,  0.2563,\n",
      "          0.9951,  0.9953,  0.2733, -0.2963,  0.0275, -0.9792, -0.9666,  0.7690,\n",
      "          0.1806, -1.0000,  0.9998, -0.9957,  0.9939,  0.9733, -0.9956,  0.8619,\n",
      "          0.0996, -0.9711, -0.0206,  0.9999,  0.9853,  0.0157,  0.2087,  0.9491,\n",
      "         -0.2971,  0.6897, -0.8493, -0.6673,  0.1326, -0.9304,  0.9936,  0.8358,\n",
      "         -0.9916,  0.9959,  0.0908,  0.7795, -0.7899,  0.8456,  0.9915, -0.0555,\n",
      "         -0.3607,  0.1068, -0.3570, -0.9824,  0.1523, -0.9977, -0.4206,  0.9236,\n",
      "          0.9909, -0.9927,  0.9940, -0.1118,  0.9263, -0.9980,  1.0000, -0.9981,\n",
      "          0.0463,  0.7851, -0.9300, -0.6443,  0.9933,  0.9893,  0.9833, -0.8166,\n",
      "         -0.6614,  0.9016,  0.9773, -0.9779, -0.0038, -0.9993, -0.7618,  0.9961,\n",
      "          0.9969,  0.0721, -0.0559, -0.9979,  0.9633, -0.8801, -0.9220, -0.0102,\n",
      "         -0.8683,  0.8339,  0.9981, -0.6305,  0.7133,  0.1007, -0.9902,  0.9125,\n",
      "          0.8141,  0.9998, -0.9708,  0.5041,  0.9905, -0.2101, -0.6762,  0.5851,\n",
      "          0.9991, -0.9840, -0.2196, -0.9995,  0.0560, -0.8280,  0.0226, -0.5787,\n",
      "          0.1189, -0.8626,  0.9726,  0.2834,  0.6924, -0.4288,  0.9812, -0.2073,\n",
      "         -0.0424, -0.2898, -0.2404,  0.5216,  0.3019,  0.9865, -0.9764,  0.9996,\n",
      "         -0.1856, -1.0000, -0.9968, -0.7735, -0.9995,  0.8171, -0.9961,  0.9888,\n",
      "          0.9381, -0.9982, -0.9991, -0.9984, -0.9953,  0.8696,  0.7189, -0.0474,\n",
      "          0.2841,  0.1023,  0.0839, -0.1844, -0.0775, -0.9630, -0.6930, -0.9984,\n",
      "          0.7365, -1.0000, -0.7391,  0.9973, -0.9969, -0.9308, -0.9190, -0.9295,\n",
      "         -0.8838,  0.4410,  0.9880, -0.0187, -0.6419, -0.9995,  0.9888, -0.8774,\n",
      "          0.1735, -0.8565, -0.9785,  0.9996,  0.8970, -0.2895, -0.0658, -0.9987,\n",
      "          0.9915, -0.9651, -0.9112, -0.9825,  0.0925, -0.9517, -0.9998,  0.0923,\n",
      "          0.9963,  0.9959,  0.9816,  0.3076, -0.3356, -0.9621,  0.1017, -0.9999,\n",
      "          0.8803,  0.8737, -0.9860, -0.8302,  0.9945,  0.9807, -0.9648, -0.9789,\n",
      "          0.9454,  0.5919,  0.9668, -0.5105, -0.4244,  0.2924,  0.0060, -0.9896,\n",
      "         -0.9208,  0.9962, -0.9992,  0.9809,  0.9953,  0.9985, -0.1679,  0.0150,\n",
      "         -0.9890, -0.9909, -0.6277,  0.3568, -0.9999,  0.9999, -1.0000,  0.6964,\n",
      "         -0.7520,  0.9213,  0.9897, -0.4849, -0.9999, -0.9997,  0.3884, -0.0283,\n",
      "          0.9908,  0.3553,  0.2244, -0.6663, -0.1636,  0.9970, -0.8853, -0.8285,\n",
      "         -0.9980,  0.9996,  0.7000, -0.9986,  0.9966, -0.9995,  0.9150,  0.9802,\n",
      "          0.8662,  0.9815, -0.9989,  1.0000, -0.9997,  0.9982, -1.0000, -0.9989,\n",
      "          0.9998, -0.9920, -0.7686, -0.9997, -0.9981,  0.8270,  0.1291, -0.4959,\n",
      "          0.9908, -0.9998, -0.9986,  0.2916, -0.9502, -0.8741,  0.9959, -0.6499,\n",
      "          0.9930, -0.1374,  0.9710,  0.3020,  0.9975,  0.9959, -0.8716, -0.7342,\n",
      "         -0.9945,  0.9853, -0.6357,  0.3316,  0.9599,  0.0985, -0.6930,  0.3366,\n",
      "         -0.9972,  0.3664, -0.2543,  0.9328,  0.9309,  0.8489, -0.0991, -0.6304,\n",
      "         -0.2366, -0.9945,  0.5581, -0.9995,  0.9809, -0.9585,  0.0370, -0.4534,\n",
      "          0.4268, -0.9751,  0.9995,  0.9985, -0.9970,  0.0906,  0.9903, -0.8157,\n",
      "          0.9807, -0.9932,  0.0276,  0.9645, -0.7478,  0.9846,  0.2325, -0.0375,\n",
      "          0.9723, -0.9960, -0.9342, -0.7086,  0.3570,  0.2058, -0.9737,  0.0578,\n",
      "          0.9894, -0.2040, -0.9997,  0.9472, -0.9994, -0.1139,  0.9779, -0.0376,\n",
      "          0.9999, -0.7879,  0.1263,  0.1961, -0.9998, -0.9990,  0.0232, -0.1049,\n",
      "         -0.9704,  0.9992, -0.0150,  0.9188, -0.9998,  0.2326,  0.9960,  0.2835,\n",
      "          0.8402, -0.8198, -0.9660, -0.9739, -0.6589,  0.0217,  0.9277, -0.9906,\n",
      "         -0.8521, -0.8565,  0.9999, -0.9977, -0.9001, -0.9882,  0.5337,  0.9101,\n",
      "          0.4539,  0.0652, -0.9437,  0.9551, -0.9458,  0.9969, -0.9957, -0.9967,\n",
      "          0.9998,  0.6827, -0.9952, -0.0930, -0.3990,  0.2730,  0.1855,  0.7937,\n",
      "         -0.9512, -0.1451, -0.9982,  0.8335, -0.9059, -0.9916, -0.5660, -0.3786,\n",
      "         -0.9948,  0.9950,  0.9684,  0.9999, -0.9998,  0.8827,  0.0571,  0.9992,\n",
      "          0.0307, -0.6629,  0.9244,  0.9995, -0.7324,  0.8856, -0.0962,  0.0249,\n",
      "          0.1881, -0.6435,  0.9969, -0.9556,  0.1620, -0.9747, -0.9999,  0.9999,\n",
      "         -0.0483,  0.9909,  0.2915,  0.8657, -0.8983,  0.9826, -0.9785, -0.9122,\n",
      "         -1.0000,  0.1611, -0.9973, -0.9922,  0.0735,  0.9890, -0.9996, -0.9930,\n",
      "         -0.4599, -1.0000,  0.9500, -0.9905, -0.8983, -0.9920,  0.9980, -0.4508,\n",
      "         -0.6785,  0.9812, -0.9808,  0.9599,  0.9523, -0.2220,  0.2329,  0.0582,\n",
      "         -0.7999, -0.9945, -0.9295, -0.9625,  0.8767, -0.9908, -0.8858,  0.9968,\n",
      "          0.9907, -0.9991, -0.9962,  0.9964, -0.0984,  0.9925, -0.5625, -0.9998,\n",
      "         -0.9999,  0.1097, -0.0143,  0.9962, -0.3277,  0.9854,  0.7772, -0.4981,\n",
      "          0.5027, -0.5811, -0.2844, -0.4310, -0.1575,  1.0000, -0.8537,  0.9935]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sample_row = toxiccomments_df.iloc[31]\n",
    "sample_comment = sample_row.comment_text\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "encoded_input = tokenizer(sample_comment, return_tensors='pt')\n",
    "output = bert_model(**encoded_input)\n",
    "\n",
    "\n",
    "##This is what highly contextualized word embeddings look like \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f081180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class handles data loading processes for training, validation, and testing\n",
    "class ToxicCommentDataModule(pl.LightningDataModule):\n",
    "\n",
    "  def __init__(self, train_df, val_df, test_df, tokenizer, batch_size=8, max_token_len=128):\n",
    "    super().__init__()\n",
    "    self.batch_size = batch_size # Define batch size for DataLoader\n",
    "    self.train_df = train_df # Dataframe for training data\n",
    "    self.val_df = val_df # Dataframe for validation data\n",
    "    self.test_df = test_df # Dataframe for test data\n",
    "    self.tokenizer = tokenizer # Tokenizer for text data\n",
    "    self.max_token_len = max_token_len # Maximum token length for tokenizer\n",
    "\n",
    "  def setup(self, stage=None):\n",
    "    # Create datasets for training, validation, and testing\n",
    "    self.train_dataset = ToxicCommentsDataset(\n",
    "      self.train_df,\n",
    "      self.tokenizer,\n",
    "      self.max_token_len\n",
    "    )\n",
    "\n",
    "    self.val_dataset = ToxicCommentsDataset(\n",
    "            self.val_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "    )\n",
    "\n",
    "    self.test_dataset = ToxicCommentsDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "    )\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    # DataLoader for training data with shuffling\n",
    "    return DataLoader(\n",
    "      self.train_dataset,\n",
    "      batch_size=self.batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=1 # Number of workers for loading data\n",
    "    )\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    # DataLoader for validation data\n",
    "    return DataLoader(\n",
    "      self.val_dataset,\n",
    "      batch_size=self.batch_size,\n",
    "      num_workers=1 # Number of workers for loading data\n",
    "    )\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    # DataLoader for test data\n",
    "    return DataLoader(\n",
    "      self.test_dataset,\n",
    "      batch_size=self.batch_size,\n",
    "      num_workers=1 # Number of workers for loading data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9c9d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "data_module = ToxicCommentDataModule(\n",
    "  train_df,\n",
    "  val_df, test_df,\n",
    "  tokenizer, batch_size = BATCH_SIZE)\n",
    "\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae3b492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb50d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentTagger(pl.LightningModule):\n",
    "  def __init__(self, n_classes: int, steps_per_epoch=None, n_epochs=None):\n",
    "    super().__init__()\n",
    "    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True) # Load pretrained BERT model\n",
    "    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes) # Define a classifier layer\n",
    "    self.steps_per_epoch = steps_per_epoch # Set steps per epoch for training\n",
    "    self.n_epochs = n_epochs # Set number of epochs for training\n",
    "    self.criterion = nn.BCELoss() # Binary Cross Entropy loss for binary classification\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels=None):\n",
    "    output = self.bert(input_ids, attention_mask=attention_mask) # BERT model processing\n",
    "    output = self.classifier(output.pooler_output) # Apply classifier\n",
    "    output = torch.sigmoid(output) # Apply sigmoid activation\n",
    "    loss = 0\n",
    "    if labels is not None:\n",
    "        loss = self.criterion(output, labels) # Calculate loss if labels are provided\n",
    "    return loss, output\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    # Extract input IDs, attention masks, and labels from batch\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels) # Perform a forward pass\n",
    "    self.log(\"train_loss\", loss, prog_bar=True, logger=True) # Log training loss\n",
    "    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    # Same as training_step, but for validation data\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"val_loss\", loss, prog_bar=True, logger=True) # Log validation loss\n",
    "    return loss\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    # Same as training_step, but for test data\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"test_loss\", loss, prog_bar=True, logger=True) # Log test loss\n",
    "    return loss\n",
    "\n",
    "  def training_epoch_end(self, outputs):\n",
    "    # Process at the end of each training epoch\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for output in outputs:\n",
    "      for out_labels in output[\"labels\"].detach().cpu():\n",
    "        labels.append(out_labels)\n",
    "      for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "        predictions.append(out_predictions)\n",
    "\n",
    "    labels = torch.stack(labels).int()\n",
    "    predictions = torch.stack(predictions)\n",
    "\n",
    "    # Calculate and log ROC AUC for each class\n",
    "    for i, name in enumerate(labels_c):\n",
    "      class_roc_auc = auroc(predictions[:, i], labels[:, i])\n",
    "      self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = AdamW(self.parameters(), lr=2e-5) # Define optimizer\n",
    "\n",
    "    # Scheduler settings for learning rate adjustment\n",
    "    warmup_steps = self.steps_per_epoch // 3\n",
    "    total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "      optimizer,\n",
    "      num_warmup_steps=warmup_steps, \n",
    "      num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    scheduler_config = {\n",
    "        'scheduler': scheduler,\n",
    "        'interval': 'step',  # 'step' or 'epoch' based on preference\n",
    "        'frequency': 1\n",
    "    }\n",
    "\n",
    "    return {'optimizer': optimizer, 'lr_scheduler': scheduler_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15a03ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##model = ToxicCommentTagger(n_classes = 6, steps_per_epoch = len(train_df) // BATCH_SIZE, n_epochs = N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3b7676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##%load_ext tensorboard\n",
    "##%tensorboard --logdir ./lightning_logs\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"toxic-comments_tl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86de895a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1395"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e053bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming ToxicCommentTagger, train_df, BATCH_SIZE, N_EPOCHS, logger, and data_module are defined\n",
    "\n",
    "# Initialize your model\n",
    "model = ToxicCommentTagger(n_classes=6, steps_per_epoch=len(train_df) // BATCH_SIZE, n_epochs=N_EPOCHS)\n",
    "\n",
    "# Add LearningRateMonitor to the callbacks\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')  # or 'epoch'\n",
    "\n",
    "# Initialize Trainer with the LearningRateMonitor callback\n",
    "trainer = pl.Trainer(    logger=logger,    callbacks=[lr_monitor],    max_epochs=N_EPOCHS,    accelerator='gpu',    devices=1)\n",
    "\n",
    "# Fit the model\n",
    "##trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1a6dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ckpt_file_path = \"C:\\\\Users\\\\salil\\\\lightning_logs\\\\toxic-comments_d\\\\version_0\\\\checkpoints\\\\epoch=11-step=11124.ckpt\"\n",
    "\n",
    "\n",
    "##trained_model = ToxicCommentTagger.load_from_checkpoint(ckpt_file_path, n_classes=len(labels_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "077a3cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1644"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(train_df) +len(test_df)) // 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dda348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Additional Validation Split, and Dataset added, cuda code added, \n",
    "\n",
    "##Code snippets were taken from here:  https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16bcfebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MAX_TOKEN_COUNT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 20\u001b[0m\n\u001b[0;32m     14\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m trained_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ToxicCommentsDataset(\n\u001b[0;32m     18\u001b[0m   val_df,\n\u001b[0;32m     19\u001b[0m   tokenizer,\n\u001b[1;32m---> 20\u001b[0m   max_token_len\u001b[38;5;241m=\u001b[39m\u001b[43mMAX_TOKEN_COUNT\u001b[49m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MAX_TOKEN_COUNT' is not defined"
     ]
    }
   ],
   "source": [
    "ckpt_file_path = \"C:\\\\Users\\\\salil\\\\lightning_logs\\\\toxic-comments_c\\\\version_0\\\\checkpoints\\\\epoch=11-step=11124.ckpt\"\n",
    "\n",
    "\n",
    "trained_model = ToxicCommentTagger.load_from_checkpoint(ckpt_file_path, n_classes=len(labels_c))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##import torch\n",
    "##from tqdm import tqdm\n",
    "\n",
    "# Assuming the rest of your code and the necessary imports are already in place\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "val_dataset = ToxicCommentsDataset(\n",
    "  val_df,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "# Wrap the prediction part of the code with torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    for item in tqdm(val_dataset):\n",
    "        _, prediction = trained_model(\n",
    "            item[\"input_ids\"].unsqueeze(dim=0).to(device), \n",
    "            item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "        )\n",
    "        predictions.append(prediction.flatten())\n",
    "        labels.append(item[\"labels\"].int())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "labels = torch.stack(labels).detach().cpu()\n",
    "\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "y_pred = predictions.numpy()\n",
    "y_true = labels.numpy()\n",
    "\n",
    "upper, lower = 1, 0\n",
    "\n",
    "y_pred = np.where(y_pred > THRESHOLD, upper, lower)\n",
    "\n",
    "\n",
    "\n",
    "average_method = 'macro'  # or 'micro', 'weighted', 'samples' depending on your need\n",
    "\n",
    "# Compute various metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=average_method, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average=average_method, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average=average_method, zero_division=0)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_true, y_pred, target_names=labels_c, zero_division=0))\n",
    "\n",
    "# Print other metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision ({average_method} average): {precision:.2f}\")\n",
    "print(f\"Recall ({average_method} average): {recall:.2f}\")\n",
    "print(f\"F1 Score ({average_method} average): {f1:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1757031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ckpt_file_path = \"C:\\\\Users\\\\salil\\\\lightning_logs\\\\toxic-comments_da\\\\version_0\\\\checkpoints\\\\epoch=14-step=27675.ckpt\"\n",
    "\n",
    "\n",
    "trained_model = ToxicCommentTagger.load_from_checkpoint(ckpt_file_path, n_classes=len(labels_c))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming the rest of your code and the necessary imports are already in place\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "val_dataset = ToxicCommentsDataset(\n",
    "  val_df,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "# Wrap the prediction part of the code with torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    for item in tqdm(val_dataset):\n",
    "        _, prediction = trained_model(\n",
    "            item[\"input_ids\"].unsqueeze(dim=0).to(device), \n",
    "            item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "        )\n",
    "        predictions.append(prediction.flatten())\n",
    "        labels.append(item[\"labels\"].int())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "labels = torch.stack(labels).detach().cpu()\n",
    "\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "y_pred = predictions.numpy()\n",
    "y_true = labels.numpy()\n",
    "\n",
    "upper, lower = 1, 0\n",
    "\n",
    "y_pred = np.where(y_pred > THRESHOLD, upper, lower)\n",
    "\n",
    "# Compute various metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=average_method, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average=average_method, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average=average_method, zero_division=0)\n",
    "\n",
    "\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_true, y_pred, target_names=labels_c, zero_division=0))\n",
    "\n",
    "# Print other metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision ({average_method} average): {precision:.2f}\")\n",
    "print(f\"Recall ({average_method} average): {recall:.2f}\")\n",
    "print(f\"F1 Score ({average_method} average): {f1:.2f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_file_path = \"C:\\\\Users\\\\salil\\\\lightning_logs\\\\toxic-comments_tl\\\\version_0\\\\checkpoints\\\\epoch=14-step=34515.ckpt\"\n",
    "\n",
    "\n",
    "trained_model = ToxicCommentTagger.load_from_checkpoint(ckpt_file_path, n_classes=len(labels_c))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##import torch\n",
    "##from tqdm import tqdm\n",
    "\n",
    "# Assuming the rest of your code and the necessary imports are already in place\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "val_dataset = ToxicCommentsDataset(\n",
    "  val_df,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "# Wrap the prediction part of the code with torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    for item in tqdm(val_dataset):\n",
    "        _, prediction = trained_model(\n",
    "            item[\"input_ids\"].unsqueeze(dim=0).to(device), \n",
    "            item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "        )\n",
    "        predictions.append(prediction.flatten())\n",
    "        labels.append(item[\"labels\"].int())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "labels = torch.stack(labels).detach().cpu()\n",
    "\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "y_pred = predictions.numpy()\n",
    "y_true = labels.numpy()\n",
    "\n",
    "upper, lower = 1, 0\n",
    "\n",
    "y_pred = np.where(y_pred > THRESHOLD, upper, lower)\n",
    "\n",
    "\n",
    "\n",
    "average_method = 'macro'  # or 'micro', 'weighted', 'samples' depending on your need\n",
    "\n",
    "# Compute various metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=average_method, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average=average_method, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average=average_method, zero_division=0)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_true, y_pred, target_names=labels_c, zero_division=0))\n",
    "\n",
    "# Print other metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision ({average_method} average): {precision:.2f}\")\n",
    "print(f\"Recall ({average_method} average): {recall:.2f}\")\n",
    "print(f\"F1 Score ({average_method} average): {f1:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7edb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "test_toxic_comment = \"You are a mediocre individual\"\n",
    "\n",
    "\n",
    "testnice = \"have a nice day\"\n",
    "encoding = tokenizer.encode_plus(\n",
    "  testnice,\n",
    "  add_special_tokens=True,\n",
    "  max_length=512,\n",
    "  return_token_type_ids=False,\n",
    "  padding=\"max_length\",\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")\n",
    "\n",
    "##_, test_prediction = trained_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "##test_prediction = test_prediction.detach().numpy()\n",
    "\n",
    "##for label, prediction in zip(labels_c, test_prediction):\n",
    "  ##print(f\"{label}: {prediction}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
